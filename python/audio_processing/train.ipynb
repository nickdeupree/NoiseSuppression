{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n",
      "TensorFlow version: 2.16.1\n",
      "CUDA available: False\n",
      "GPU devices: []\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D,Conv1DTranspose,Concatenate,Input\n",
    "import numpy as np\n",
    "import IPython.display\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"CUDA available:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available. TensorFlow will use the CPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set it as the default device\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "  print(\"GPU is available and will be used by TensorFlow.\")\n",
    "else:\n",
    "  print(\"GPU is not available. TensorFlow will use the CPU.\")\n",
    "\n",
    "clean_sounds = glob.glob('../content/CleanData/*')\n",
    "noisy_sounds = glob.glob('../content/NoisyData/*')\n",
    "\n",
    "clean_sounds_list,_ = tf.audio.decode_wav(tf.io.read_file(clean_sounds[0]),desired_channels=1)\n",
    "for i in tqdm(clean_sounds[1:]):\n",
    "  so,_ = tf.audio.decode_wav(tf.io.read_file(i),desired_channels=1)\n",
    "  clean_sounds_list = tf.concat((clean_sounds_list,so),0)\n",
    "\n",
    "noisy_sounds_list,_ = tf.audio.decode_wav(tf.io.read_file(noisy_sounds[0]),desired_channels=1)\n",
    "for i in tqdm(noisy_sounds[1:]):\n",
    "  so,_ = tf.audio.decode_wav(tf.io.read_file(i),desired_channels=1)\n",
    "  noisy_sounds_list = tf.concat((noisy_sounds_list,so),0)\n",
    "\n",
    "clean_sounds_list.shape,noisy_sounds_list.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batching_size = 12000\n",
    "\n",
    "clean_train,noisy_train = [],[]\n",
    "\n",
    "for i in tqdm(range(0,clean_sounds_list.shape[0]-batching_size,batching_size)):\n",
    "  clean_train.append(clean_sounds_list[i:i+batching_size])\n",
    "  noisy_train.append(noisy_sounds_list[i:i+batching_size])\n",
    "\n",
    "clean_train = tf.stack(clean_train)\n",
    "noisy_train = tf.stack(noisy_train)\n",
    "\n",
    "clean_train.shape,noisy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(x_train,y_train):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))\n",
    "  dataset = dataset.shuffle(100).batch(64,drop_remainder=True)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(noisy_train[:40000],clean_train[:40000])\n",
    "test_dataset = get_dataset(noisy_train[40000:],clean_train[40000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveplot(np.squeeze(clean_train[5].numpy(),axis=-1))\n",
    "plt.show()\n",
    "librosa.display.waveplot(np.squeeze(noisy_train[5].numpy(),axis=-1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(batching_size,1))\n",
    "c1 = Conv1D(2,32,2,'same',activation='relu')(inp)\n",
    "c2 = Conv1D(4,32,2,'same',activation='relu')(c1)\n",
    "c3 = Conv1D(8,32,2,'same',activation='relu')(c2)\n",
    "c4 = Conv1D(16,32,2,'same',activation='relu')(c3)\n",
    "c5 = Conv1D(32,32,2,'same',activation='relu')(c4)\n",
    "\n",
    "dc1 = Conv1DTranspose(32,32,1,padding='same')(c5)\n",
    "conc = Concatenate()([c5,dc1])\n",
    "dc2 = Conv1DTranspose(16,32,2,padding='same')(conc)\n",
    "conc = Concatenate()([c4,dc2])\n",
    "dc3 = Conv1DTranspose(8,32,2,padding='same')(conc)\n",
    "conc = Concatenate()([c3,dc3])\n",
    "dc4 = Conv1DTranspose(4,32,2,padding='same')(conc)\n",
    "conc = Concatenate()([c2,dc4])\n",
    "dc5 = Conv1DTranspose(2,32,2,padding='same')(conc)\n",
    "conc = Concatenate()([c1,dc5])\n",
    "dc6 = Conv1DTranspose(1,32,2,padding='same')(conc)\n",
    "conc = Concatenate()([inp,dc6])\n",
    "dc7 = Conv1DTranspose(1,32,1,padding='same',activation='linear')(conc)\n",
    "model = tf.keras.models.Model(inp,dc7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,show_shapes=True,show_layer_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.002),loss=tf.keras.losses.MeanAbsoluteError())\n",
    "history = model.fit(train_dataset,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(np.squeeze(noisy_train[22].numpy()),rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(tf.squeeze(model.predict(tf.expand_dims(tf.expand_dims(noisy_train[22],-1),0))),rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('NoiseSuppressionModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TFLite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model\n",
    "with open('TFLiteModel.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
